{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# deberta-v3-small Experiments\n\nWe use fast experiments with 1/5 of the data and 3 epochs to quickly determine which adaptations are helpful and report the cv-score.  \nFor adaptation we want to explore further, we retrain on the whole dataset and evaluate on the leader board.  \n\n</br> </br>  \n\n\n## Fast Experiments \n*1/5 of original training data, 3 epochs, 4 fold cv*\n\n---\n\n### Prompt Engineering\n|     Model              |  CFG | cv score | lb score | comment|\n|:----------------------|:--------|:--------:|:--------:|:--------|\n|  Baseline           | `fast_baseline_cfg`   | 0.7430   | - | |\n|  CPC Context Text   | `fast_context_cfg`    | 0.7523    | - | |  \n|  Custom Tokens      | `fast_customtok_cfg`  | 0.7464  | - | |  \n\n\n\n**Conclusion**: Adding Context Text seems to work best here, we will hence continue with that.\n</br> </br>  \n\n### Model Type\n    \n|     Model              |  CFG | cv score | lb score | comment|\n|:----------------------|:--------|:--------:|:--------:|:--------|\n|  Regression       | `fast_reg_cfg` |  *see CPC Context Text*  | - | |\n|  Classification   | `fast_class_cfg` | 0.7378 | - | |  \n|  Ordinal          | `fast_ord_cfg` | 0.7262 | - | |  \n\n\n**Conclusion**: Regular regression seems to work best, we will hence continue with that.\n</br> </br>  \n\n### Classical NLP Preprocssing\n|     Model              |  CFG | cv score | lb score | comment|\n|:----------------------|:--------|:--------:|:--------:|:--------|\n| Stemming              | `fast_stem_cfg`   |  0.6663 | - | |\n| Lemmatizing           | `fast_lemma_cfg`   | 0.7510  | - | |\n| Special Characters    | `fast_specchr_cfg`   |   | - | Removing special characters from the prompt |\n\n\n### Postprocessing\n|     Model              |  CFG | cv score | lb score | comment|\n|:----------------------|:--------|:--------:|:--------:|:--------|\n| Clipping              | `fast_post_clip_cfg`     |  0.7423 | - | Range `[0,1]`|\n| MinMax                | `fast_post_minmax_cfg`   |  0.7523 | - | Range `[0,1]`|\n| Chemical Lookup       | `fast_post_chem_cfg`     |  0.7541 | - |              |\n\n**Conclusion**: `Clipping` performs a little worse, which makes sense when considering the pearson correlation score.\n`MinMax` performs exactly the same, which is again not surprising.\n`Chemical Lookup` performs a little better, we hence continue with that.\n</br> </br>  \n\n### Data Augmentation\nTo aquire a fair estimate, here, we only validate on non-augmented data and keep the training data the same size as in previous fast experiments.  \n\n\n|     Model              |  CFG | cv score | lb score | comment|\n|:----------------------|:--------|:--------:|:--------:|:--------|\n| Identities              |   `fast_aug_ident_cfg` |  0.7456 | - | Adds the reverse mapping for Anchor-Target pairs with score 1|\n| All Mirrored            |   `fast_aug_mirr_cfg` |  0.7382 | - | Adds the reverse mapping for all Anchor-Target pairs |\n| Identity Paths          |  `fast_aug_identpaths_cfg` | 0.7597  | - | Adds all pairs in a path between phrases connected by a score of 1|\n| All Mirrored + Identity Paths |  `fast_aug_mirridentpaths_cfg` | 0.7449 | - | Adds identity paths and additionally mirrors all Anchor-Target pairs|\n| Neighbors               |  `fast_aug_neighbors_cfg` | 0.7518 | - | Additionally to *All Mirrored + Identity Paths* considers phrases adjacent of idendity paths.|\n| Chemical Compounds      |   `fast_aug_chem_cfg`  | 0.7576 | - | Finds synonyms for formulae of common chemical compounds in the dataset and creates new phrases from it.| \n\n**Conclusion**: Augmenting seems not to hurt performance significantly and it is likely that the additional data will increase performance.  \nTo giver a better comparison we will compare full models based on the leaderboard score.\n\n</br>  </br>  </br>  \n\n## Full Experiments \n*full training data, 5 epochs, 4 fold cv*\n\n---\n\n    \n|     Model              |  CFG | cv score | lb (pb) score | comment|\n|:----------------------|:--------|:--------:|:--------:|:--------|\n|  Baseline                        | `full_baseline_cfg`      | 0.8314  | 0.8210 (0.8107) | |\n|  Context Text                    | `full_ctxt_cfg`          | 0.8337  | 0.8293 (0.8171) | |  \n|  Chemical Lookup                 | `full_post_chem_cfg`     | 0.8361  | 0.8292 (0.8167) | |\n|  Neighbors Augmentation          | `full_aug_neighbors_cfg` |   |  | |     \n|  Chemical Compounds Augmentation | `full_aug_chem_cfg`      |   |  | | \n\n","metadata":{}},{"cell_type":"markdown","source":"# Directory settings","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nINPUT_DIR = '../input/us-patent-phrase-to-phrase-matching/'\nIDENTITY_MAPPINGS_DIR = '../input/uspppm-identity-mappings/'\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"id":"fa3b873b","papermill":{"duration":0.039998,"end_time":"2022-03-21T11:33:53.291434","exception":false,"start_time":"2022-03-21T11:33:53.251436","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T07:19:24.512739Z","iopub.execute_input":"2022-06-30T07:19:24.513300Z","iopub.status.idle":"2022-06-30T07:19:24.518687Z","shell.execute_reply.started":"2022-06-30T07:19:24.513248Z","shell.execute_reply":"2022-06-30T07:19:24.517824Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Model Configuration","metadata":{"id":"1d0c4430","papermill":{"duration":0.02483,"end_time":"2022-03-21T11:33:53.341306","exception":false,"start_time":"2022-03-21T11:33:53.316476","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from dataclasses import dataclass, field\nfrom typing import Set, Optional, Tuple, Dict\n\n\n@dataclass\nclass ModelConfig:\n    ############################################\n    # Prompt engineering\n    ############################################\n    # - ctxt_txt (Default): Add context text at the end of the prompt\n    # - custom_tok: Add custom tokens for contexts and a custom separator token\n    # - None: Append the context appreviation to the end of prompt\n    # When true uses custom separator token and context tokens\n    prompt_engineering: Optional[str] = 'ctx_txt' # ['custom_tok', None]\n    custom_sep_token: str = '[S]'\n        \n    ############################################\n    # Model Type\n    ############################################\n    # when true uses classification else a regression model\n    classification: bool = False\n    # when true uses ordinal regression (only active when `classification = False`)\n    ordinal: bool = False\n    \n    ############################################\n    # Traditional NLP preprocessing\n    ############################################\n    stemming: bool = False\n    lemmatizing: bool = False\n    spec_chr_rem: bool = False\n        \n    ############################################\n    # Post Processing\n    ############################################\n    clipping: bool = False\n    minmax: bool = False\n    # Use averaging of chemical component synonyms for \n    chem_comp_pred_avg: bool  = False\n        \n    ############################################\n    # Data Augmentation\n    ############################################\n    # Use chemical component synonyms to creat samples for augmenting the training set\n    chem_comp_train_aug: bool = False\n        \n    # How to augment the data for training using the graph based indentity mappings\n    # available options ['neighbors'. paths_mirrored', 'paths', 'mirrored', 'identities', None]\n    # None indicates that no augmentation should take place\n    augment_identity_graph_data: Optional[str] = None \n    validate_on_original: bool = True\n        \n    \n    ############################################\n    # General Model Config and Hyperparams\n    ############################################\n    debug: bool = False\n    apex: bool = True\n    print_freq: int= 200\n    num_workers: int = 4\n    model: str = \"microsoft/deberta-v3-small\"\n    scheduler: str = 'cosine' # ['linear', 'cosine']\n    batch_scheduler: bool = True\n    num_cycles: float = 0.5\n    num_warmup_steps: int = 0\n    encoder_lr: float = 2e-5\n    decoder_lr: float = 2e-5\n    min_lr: float = 1e-6\n    eps: float = 1e-6\n    betas: Set[float] = (0.9, 0.999)\n    batch_size: int = 16\n    fc_dropout: float = 0.2\n    max_len: int = 512\n    weight_decay: float = 0.01\n    gradient_accumulation_steps: int = 1\n    max_grad_norm: int = 1000\n    seed: int = 42\n    \n    epochs: int = 5\n    train_frac: Optional[float] = None\n    n_fold: int = 4\n    trn_fold: Set[int] = (0, 1, 2, 3)\n    map_score: Dict[float, int] = field(default_factory = lambda: ({0.0: 0, 0.25: 1, 0.5: 2, 0.75: 3, 1.0: 4}))\n    map_labels: Dict[int, float] = field(default_factory = lambda: ({0: 0.0, 1: 0.25, 2: 0.5, 3: 0.75, 4: 1.0}))\n        \n    target_size: int=1\n    def __post_init__(self):\n        if self.classification or self.ordinal:\n            self.target_size = 5\n        else:\n            self.target_size = 1  \n\n\n############################################\n# Experiment Configurations\n############################################\n\n# Fast Experiments\nFAST_BASE_TRAIN_FRAC = 1/5\nFAST_BASE_TRAIN_EPOCHS = 3\n\ndef fastModelConfig(**kwargs):\n    return ModelConfig(**kwargs, train_frac=FAST_BASE_TRAIN_FRAC, epochs=FAST_BASE_TRAIN_EPOCHS)\n\n\nfast_baseline_cfg = fastModelConfig(prompt_engineering=None)\nfast_context_ctx = fastModelConfig()\nfast_customtok_cfg = fastModelConfig(prompt_engineering='custom_tok')\n\nfast_reg_cfg = fastModelConfig()\nfast_class_cfg = fastModelConfig(classification=True)\nfast_ord_cfg = fastModelConfig(ordinal=True)\n\nfast_stem_cfg = fastModelConfig(stemming=True)\nfast_lemma_cfg = fastModelConfig(lemmatizing=True)\nfast_specchr_cfg = fastModelConfig(spec_chr_rem=True)\n\nfast_aug_ident_cfg = fastModelConfig(augment_identity_graph_data='identities')\nfast_aug_mirr_cfg = fastModelConfig(augment_identity_graph_data='mirrored')\nfast_aug_identpaths_cfg = fastModelConfig(augment_identity_graph_data='paths')\nfast_aug_mirridentpaths_cfg = fastModelConfig(augment_identity_graph_data='paths_mirrored')\nfast_aug_neighbors_cfg = fastModelConfig(augment_identity_graph_data='neighbors')\nfast_aug_chem_cfg = fastModelConfig(chem_comp_train_aug=True)\n\nfast_post_clip_cfg = fastModelConfig(clipping=True)\nfast_post_minmax_cfg = fastModelConfig(minmax=True)\nfast_post_chem_cfg = fastModelConfig(chem_comp_pred_avg=True)\n\n# Full Experiments\nfull_baseline_cfg = ModelConfig(prompt_engineering=None)\nfull_ctxt_cfg = ModelConfig()\nfull_post_chem_cfg = ModelConfig(chem_comp_pred_avg=True)\nfull_aug_neighbors_cfg = ModelConfig(augment_identity_graph_data='neighbors')\nfull_aug_chem_cfg = ModelConfig(chem_comp_train_aug=True)\n","metadata":{"id":"48dd82bb","papermill":{"duration":0.03584,"end_time":"2022-03-21T11:33:53.402377","exception":false,"start_time":"2022-03-21T11:33:53.366537","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T07:19:24.622472Z","iopub.execute_input":"2022-06-30T07:19:24.622975Z","iopub.status.idle":"2022-06-30T07:19:24.649581Z","shell.execute_reply.started":"2022-06-30T07:19:24.622923Z","shell.execute_reply":"2022-06-30T07:19:24.648468Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Config used throughout this notebook","metadata":{}},{"cell_type":"code","source":"############################################\n# Fast Experiments\n############################################\n\n# CFG = fast_baseline_cfg\n# CFG = fast_context_ctx\n# CFG = fast_customtok_cfg\n\n# CFG = fast_reg_cfg\n# CFG = fast_class_cfg\n# CFG = fast_ord_cfg\n\n# CFG = fast_stem_cfg\n# CFG = fast_lemma_cfg\nCFG = fast_specchr_cfg\n\n# CFG = fast_aug_ident_cfg\n# CFG = fast_aug_mirr_cfg\n# CFG = fast_aug_identpaths_cfg\n# CFG = fast_aug_mirridentpaths_cfg\n# CFG = fast_aug_neighbors_cfg\n# CFG = fast_aug_chem_cfg\n\n# CFG = fast_post_clip_cfg\n# CFG = fast_post_minmax_cfg\n# CFG = fast_post_chem_cfg\n\n############################################\n# Full Experiments\n############################################\n# CFG = full_baseline_cfg\n# CFG = full_ctxt_cfg\n# CFG = full_post_chem_cfg\n# CFG = full_aug_neighbors_cfg\n# CFG = full_aug_chem_cfg","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:19:24.704379Z","iopub.execute_input":"2022-06-30T07:19:24.705111Z","iopub.status.idle":"2022-06-30T07:19:24.710093Z","shell.execute_reply.started":"2022-06-30T07:19:24.705057Z","shell.execute_reply":"2022-06-30T07:19:24.709370Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Library","metadata":{"id":"f2ed8ef2","papermill":{"duration":0.024963,"end_time":"2022-03-21T11:33:53.573248","exception":false,"start_time":"2022-03-21T11:33:53.548285","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport shutil\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.preprocessing import minmax_scale\n    \nimport torch\nprint(f\"torch.__version__: {torch.__version__}\")\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nos.system('pip uninstall -y transformers')\nos.system('pip uninstall -y tokenizers')\nos.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels transformers')\nos.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels tokenizers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.data.path.append('../input/wordnet')\n\n\n# common chemical formulae lookup utility script\nfrom uspppm_common_chemical_compound_lookup import USPPPMChemCompLookup","metadata":{"executionInfo":{"elapsed":20123,"status":"ok","timestamp":1644920080956,"user":{"displayName":"Yasufumi Nakama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17486303986134302670"},"user_tz":-540},"id":"35916341","outputId":"06fa0ab8-a380-4f54-a98d-b7015b79d9e2","papermill":{"duration":27.238641,"end_time":"2022-03-21T11:34:20.895812","exception":false,"start_time":"2022-03-21T11:33:53.657171","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T07:19:24.777209Z","iopub.execute_input":"2022-06-30T07:19:24.777668Z","iopub.status.idle":"2022-06-30T07:20:02.983549Z","shell.execute_reply.started":"2022-06-30T07:19:24.777634Z","shell.execute_reply":"2022-06-30T07:20:02.982079Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"torch.__version__: 1.9.1+cpu\nFound existing installation: transformers 4.16.2\nUninstalling transformers-4.16.2:\n  Successfully uninstalled transformers-4.16.2\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"},{"name":"stdout","text":"Found existing installation: tokenizers 0.11.6\nUninstalling tokenizers-0.11.6:\n  Successfully uninstalled tokenizers-0.11.6\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"},{"name":"stdout","text":"Looking in links: ../input/pppm-pip-wheels\nProcessing /kaggle/input/pppm-pip-wheels/transformers-4.18.0-py3-none-any.whl\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.4.0)\nProcessing /kaggle/input/pppm-pip-wheels/tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.49)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nInstalling collected packages: tokenizers, transformers\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.9.1 requires transformers<4.17,>=4.1, but you have transformers 4.18.0 which is incompatible.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"},{"name":"stdout","text":"Successfully installed tokenizers-0.12.1 transformers-4.18.0\nLooking in links: ../input/pppm-pip-wheels\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.12.1)\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"},{"name":"stdout","text":"tokenizers.__version__: 0.12.1\ntransformers.__version__: 4.18.0\nenv: TOKENIZERS_PARALLELISM=true\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Utils","metadata":{"id":"fd586614","papermill":{"duration":0.029385,"end_time":"2022-03-21T11:34:21.160041","exception":false,"start_time":"2022-03-21T11:34:21.130656","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Chemical Compound Lookup","metadata":{}},{"cell_type":"code","source":"chem_lookup = USPPPMChemCompLookup(chem_comp_path='../input/chemical-compounds-lookup/compounds.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:20:02.985784Z","iopub.execute_input":"2022-06-30T07:20:02.986072Z","iopub.status.idle":"2022-06-30T07:20:03.012584Z","shell.execute_reply.started":"2022-06-30T07:20:02.986039Z","shell.execute_reply":"2022-06-30T07:20:03.011339Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Chemical Compound Lookup Tests**","metadata":{}},{"cell_type":"code","source":"print('Testing basic formula lookup...')\nprint(chem_lookup.lookup_df)\nprint(chem_lookup.phrase_chem_formula_synonym('agbr test'))\nprint(chem_lookup.phrase_chem_formula_synonym('agbr dna test agonc ag2cl2'))\nprint(chem_lookup.phrase_chem_formula_synonym('dna test d2o'))\nprint(chem_lookup.chem_formula_synonyms('c3h6'))\nprint('Done!')\nprint()\nprint('Testing train dataset augmentation...')\nchem_test = pd.DataFrame({'id': pd.Series(['t1', 't2', 't3', 't4']),\n                          'score': pd.Series([3, 2, 1, 0]),\n                          'anchor': pd.Series(['agbr dna test agonc ag2cl2', 'agbr test', 'agbr', 'last']),\n                          'target': pd.Series(['agonc ag2cl2', 'test thingy', 'c4h7no4', 'last']),\n                          'context': pd.Series(['G02', 'G02', 'C12', 'C12'])})\nprint(\"Before\")\nprint(chem_test)\nprint(\"After\")\nprint(chem_lookup.pre_augment_chem_formulae(chem_test, True))\nprint('Done!')\nprint()\nprint('Testing test dataset augmentation...')\nchem_test = pd.DataFrame({'id': pd.Series(['t1', 't2', 't3', 't4']),\n                          'text': pd.Series([\n                              'agbr dna test agonc ag2cl2 [SEP] agonc ag2cl2 [SEP] G02',\n                              'agbr test [SEP] test thingy [SEP] G02',\n                              'agbr [SEP] c4h7no4 [SEP] C12',\n                              'last [SEP] last [SEP] C12'\n                          ])})\nprint(\"Before\")\nprint(chem_test)\nprint(\"After\")\nprint(chem_lookup.post_augment_chem_formulae(chem_test, True))\nprint('Done!')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:20:03.014350Z","iopub.execute_input":"2022-06-30T07:20:03.014700Z","iopub.status.idle":"2022-06-30T07:20:05.855031Z","shell.execute_reply.started":"2022-06-30T07:20:03.014666Z","shell.execute_reply":"2022-06-30T07:20:05.853989Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Testing basic formula lookup...\n                            Name  Formula\n0               actiniumiiioxide    ac2o3\n1     silvertetrachloroaluminate  agalcl4\n2                  silverbromide     agbr\n3                  silverbromate   agbro3\n4                  silvercyanide     agcn\n...                          ...      ...\n4067                zirconateion    zro32\n4068          zirconiumphosphide     zrp2\n4069            zirconiumsulfide     zrs2\n4070           zirconiumsilicide    zrsi2\n4071          zirconiumphosphate  zr3po44\n\n[4072 rows x 2 columns]\n[('silverbromide test', True)]\n[('silverbromide dna test silverfulminate disilverdichloride', True), ('silverbromide dna test silvercyanate disilverdichloride', True), ('silverbromide dna test silverfulminate silveriidichloride', True), ('silverbromide dna test silvercyanate silveriidichloride', True)]\n[('dna test deuteriumoxide', True), ('dna test heavywater', True)]\n['cyclopropane', 'propylene']\nDone!\n\nTesting train dataset augmentation...\nBefore\n   id  score                      anchor        target context\n0  t1      3  agbr dna test agonc ag2cl2  agonc ag2cl2     G02\n1  t2      2                   agbr test   test thingy     G02\n2  t3      1                        agbr       c4h7no4     C12\n3  t4      0                        last          last     C12\nAfter\n   id  score                                             anchor                              target context  augmented\n0  t1      3  silverbromide dna test silverfulminate disilve...  silverfulminate disilverdichloride     G02       True\n0  t1      3  silverbromide dna test silverfulminate disilve...    silvercyanate disilverdichloride     G02       True\n0  t1      3  silverbromide dna test silverfulminate disilve...  silverfulminate silveriidichloride     G02       True\n0  t1      3  silverbromide dna test silverfulminate disilve...    silvercyanate silveriidichloride     G02       True\n0  t1      3  silverbromide dna test silvercyanate disilverd...  silverfulminate disilverdichloride     G02       True\n0  t1      3  silverbromide dna test silvercyanate disilverd...    silvercyanate disilverdichloride     G02       True\n0  t1      3  silverbromide dna test silvercyanate disilverd...  silverfulminate silveriidichloride     G02       True\n0  t1      3  silverbromide dna test silvercyanate disilverd...    silvercyanate silveriidichloride     G02       True\n0  t1      3  silverbromide dna test silverfulminate silveri...  silverfulminate disilverdichloride     G02       True\n0  t1      3  silverbromide dna test silverfulminate silveri...    silvercyanate disilverdichloride     G02       True\n0  t1      3  silverbromide dna test silverfulminate silveri...  silverfulminate silveriidichloride     G02       True\n0  t1      3  silverbromide dna test silverfulminate silveri...    silvercyanate silveriidichloride     G02       True\n0  t1      3  silverbromide dna test silvercyanate silveriid...  silverfulminate disilverdichloride     G02       True\n0  t1      3  silverbromide dna test silvercyanate silveriid...    silvercyanate disilverdichloride     G02       True\n0  t1      3  silverbromide dna test silvercyanate silveriid...  silverfulminate silveriidichloride     G02       True\n0  t1      3  silverbromide dna test silvercyanate silveriid...    silvercyanate silveriidichloride     G02       True\n1  t2      2                                 silverbromide test                         test thingy     G02       True\n2  t3      1                                      silverbromide                        asparticacid     C12       True\n2  t3      1                                      silverbromide                                 asp     C12       True\n3  t4      0                                               last                                last     C12      False\nDone!\n\nTesting test dataset augmentation...\nBefore\n   id                                               text\n0  t1  agbr dna test agonc ag2cl2 [SEP] agonc ag2cl2 ...\n1  t2              agbr test [SEP] test thingy [SEP] G02\n2  t3                       agbr [SEP] c4h7no4 [SEP] C12\n3  t4                          last [SEP] last [SEP] C12\nAfter\n   id                                               text\n0  t1  silverbromide dna test silverfulminate disilve...\n0  t1  silverbromide dna test silvercyanate disilverd...\n0  t1  silverbromide dna test silverfulminate silveri...\n0  t1  silverbromide dna test silvercyanate silveriid...\n1  t2     silverbromide test [SEP] test thingy [SEP] G02\n2  t3         silverbromide [SEP] asparticacid [SEP] C12\n2  t3                  silverbromide [SEP] asp [SEP] C12\n3  t4                          last [SEP] last [SEP] C12\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = sp.stats.pearsonr(y_true, y_pred)[0]\n    return score\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    logger.propagate = False\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"id":"d5c0ccc6","papermill":{"duration":0.041568,"end_time":"2022-03-21T11:34:21.231169","exception":false,"start_time":"2022-03-21T11:34:21.189601","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T07:20:05.857080Z","iopub.execute_input":"2022-06-30T07:20:05.857803Z","iopub.status.idle":"2022-06-30T07:20:05.872333Z","shell.execute_reply.started":"2022-06-30T07:20:05.857762Z","shell.execute_reply":"2022-06-30T07:20:05.871367Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{"id":"cb3d8e1e","papermill":{"duration":0.028943,"end_time":"2022-03-21T11:34:21.290864","exception":false,"start_time":"2022-03-21T11:34:21.261921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\norig_data = pd.read_csv(INPUT_DIR+'train.csv')\n\nif CFG.augment_identity_graph_data == 'neighbors':\n    train = pd.read_csv(IDENTITY_MAPPINGS_DIR+'all_mirrored_w_identity_path_neighbors.csv')\nelif CFG.augment_identity_graph_data == 'paths_mirrored':\n    train = pd.read_csv(IDENTITY_MAPPINGS_DIR+'all_mirrored_w_identity_paths.csv')\nelif CFG.augment_identity_graph_data == 'paths':\n    train = pd.read_csv(IDENTITY_MAPPINGS_DIR+'identity_paths_mirrored.csv')\nelif CFG.augment_identity_graph_data == 'mirrored':\n    train = pd.read_csv(IDENTITY_MAPPINGS_DIR+'all_mirrored.csv')\nelif CFG.augment_identity_graph_data == 'identities':\n    train = pd.read_csv(IDENTITY_MAPPINGS_DIR+'identity_mirrored.csv')\nelif CFG.augment_identity_graph_data == None:\n    train = orig_data\nelse:\n    raise(ValueError('CFG.augment_identity_graph_data = {} not recognized!'.format(CFG.augment_identity_graph_data)))\n\nif CFG.chem_comp_train_aug:\n    train = chem_lookup.pre_augment_chem_formulae(train).reindex()\n    \ndisplay(train)\n    \nif CFG.train_frac:\n    # to get a fair estimate we always use a fraction of the original data\n    n = int(CFG.train_frac * len(orig_data))\n    train = train.sample(n=n, replace=False, ignore_index=True)\n    \ntest = pd.read_csv(INPUT_DIR+'test.csv')\nsubmission = pd.read_csv(INPUT_DIR+'sample_submission.csv')\nprint(f\"train.shape: {train.shape}\")\nprint(f\"test.shape: {test.shape}\")\nprint(f\"submission.shape: {submission.shape}\")\n# display(train.head())\n# display(test.head())\n# display(submission.head())","metadata":{"executionInfo":{"elapsed":2627,"status":"ok","timestamp":1644920084001,"user":{"displayName":"Yasufumi Nakama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17486303986134302670"},"user_tz":-540},"id":"bef012d3","outputId":"d4d60dbc-510c-4f34-8d64-dd1d88c4808c","papermill":{"duration":0.860612,"end_time":"2022-03-21T11:34:22.180396","exception":false,"start_time":"2022-03-21T11:34:21.319784","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T07:20:05.873475Z","iopub.execute_input":"2022-06-30T07:20:05.873707Z","iopub.status.idle":"2022-06-30T07:20:06.036925Z","shell.execute_reply.started":"2022-06-30T07:20:05.873680Z","shell.execute_reply":"2022-06-30T07:20:06.035645Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"                     id        anchor                  target context  score\n0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50\n1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75\n2      36d72442aefd8232     abatement         active catalyst     A47   0.25\n3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50\n4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00\n...                 ...           ...                     ...     ...    ...\n36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00\n36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50\n36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50\n36471  756ec035e694722b  wood article         wooden material     B44   0.75\n36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50\n\n[36473 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36d72442aefd8232</td>\n      <td>abatement</td>\n      <td>active catalyst</td>\n      <td>A47</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5296b0c19e1ce60e</td>\n      <td>abatement</td>\n      <td>eliminating process</td>\n      <td>A47</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54c1e3b9184cb5b6</td>\n      <td>abatement</td>\n      <td>forest region</td>\n      <td>A47</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36468</th>\n      <td>8e1386cbefd7f245</td>\n      <td>wood article</td>\n      <td>wooden article</td>\n      <td>B44</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>36469</th>\n      <td>42d9e032d1cd3242</td>\n      <td>wood article</td>\n      <td>wooden box</td>\n      <td>B44</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>36470</th>\n      <td>208654ccb9e14fa3</td>\n      <td>wood article</td>\n      <td>wooden handle</td>\n      <td>B44</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>36471</th>\n      <td>756ec035e694722b</td>\n      <td>wood article</td>\n      <td>wooden material</td>\n      <td>B44</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>36472</th>\n      <td>8d135da0b55b8c88</td>\n      <td>wood article</td>\n      <td>wooden substrate</td>\n      <td>B44</td>\n      <td>0.50</td>\n    </tr>\n  </tbody>\n</table>\n<p>36473 rows × 5 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"train.shape: (7294, 5)\ntest.shape: (36, 4)\nsubmission.shape: (36, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# For sanity check below - count the different \nsum(chem_lookup.pre_augment_chem_formulae(train)['id'].duplicated())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:20:06.038622Z","iopub.execute_input":"2022-06-30T07:20:06.038870Z","iopub.status.idle":"2022-06-30T07:20:13.009281Z","shell.execute_reply.started":"2022-06-30T07:20:06.038842Z","shell.execute_reply":"2022-06-30T07:20:13.007938Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"code","source":"# Add augmented indicator\n# I'm sorry for this dirty hack\nif 'augmented' not in train.columns:\n    train['augmented'] = train['id'].str.contains('_')\nelse:\n    train['augmented'] = train['id'].str.contains('_') | train['augmented']\n    \nprint(sum(train['augmented']), 'augmented samples')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:20:13.010724Z","iopub.execute_input":"2022-06-30T07:20:13.011497Z","iopub.status.idle":"2022-06-30T07:20:13.028235Z","shell.execute_reply.started":"2022-06-30T07:20:13.011452Z","shell.execute_reply":"2022-06-30T07:20:13.027521Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"0 augmented samples\n","output_type":"stream"}]},{"cell_type":"code","source":"# ====================================================\n# CPC Data\n# ====================================================\ndef get_cpc_texts():\n    contexts = []\n    pattern = '[A-Z]\\d+'\n    for file_name in os.listdir('../input/cpc-data/CPCSchemeXML202105'):\n        result = re.findall(pattern, file_name)\n        if result:\n            contexts.append(result)\n    contexts = sorted(set(sum(contexts, [])))\n    results = {}\n    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n        with open(f'../input/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n            s = f.read()\n        pattern = f'{cpc}\\t\\t.+'\n        result = re.findall(pattern, s)\n        cpc_result = result[0].lstrip(pattern)\n        for context in [c for c in contexts if c[0] == cpc]:\n            pattern = f'{context}\\t\\t.+'\n            result = re.findall(pattern, s)\n            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n    return results\n\n\ncpc_texts = get_cpc_texts()\ntorch.save(cpc_texts, OUTPUT_DIR+\"cpc_texts.pth\")\ntrain['context_text'] = train['context'].map(cpc_texts)\ntest['context_text'] = test['context'].map(cpc_texts)\n# display(train.head())\n# display(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:20:13.029296Z","iopub.execute_input":"2022-06-30T07:20:13.029952Z","iopub.status.idle":"2022-06-30T07:20:14.422306Z","shell.execute_reply.started":"2022-06-30T07:20:13.029881Z","shell.execute_reply":"2022-06-30T07:20:14.421382Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Tradition NLP Preprocessing","metadata":{}},{"cell_type":"code","source":"train_old = train.copy()\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef stem_phrase(phrase: str):\n    tokens = word_tokenize(phrase)\n    return \" \".join([stemmer.stem(token) for token in tokens])\n        \ndef lemmatize_phrase(phrase: str):\n    tokens = word_tokenize(phrase)\n    return \" \".join([lemmatizer.lemmatize(token) for token in tokens])\n\ndef rem_stop_words(phrase: str):\n    tokens = word_tokenize(phrase)\n    return \" \".join([t for t in tokens if t not in stop_words])\n\ndef rem_spec_chr(phrase: str):\n    return re.sub(r'[^A-Za-z0-9\\s]+', \"\", phrase)\n    \nif CFG.stemming:\n    train['anchor'] = train['anchor'].apply(stem_phrase)\n    train['target'] = train['target'].apply(stem_phrase)\n    test['anchor'] = test['anchor'].apply(stem_phrase)\n    test['target'] = test['target'].apply(stem_phrase)\n    \n    \nif CFG.lemmatizing:\n    train['anchor'] = train['anchor'].apply(lemmatize_phrase)\n    train['target'] = train['target'].apply(lemmatize_phrase)\n    test['anchor'] = test['anchor'].apply(lemmatize_phrase)\n    test['target'] = test['target'].apply(lemmatize_phrase)\n    \n\n#print(\"Nr. special characters: \", sum(train['anchor'].apply(lambda p: len(re.findall(r'[^A-Za-z0-9\\s]+', p)) > 0)))\n\nif CFG.spec_chr_rem:\n    train['anchor'] = train['anchor'].apply(lambda p: rem_spec_chr(p))\n    train['target'] = train['target'].apply(lambda p: rem_spec_chr(p))\n    train['context_text'] = train['context_text'].apply(lambda p: rem_spec_chr(p))\n    test['anchor'] = test['anchor'].apply(lambda p: rem_spec_chr(p))\n    test['target'] = test['target'].apply(lambda p: rem_spec_chr(p))\n    test['context_text'] = test['context_text'].apply(lambda p: rem_spec_chr(p))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:21:32.083019Z","iopub.execute_input":"2022-06-30T07:21:32.083657Z","iopub.status.idle":"2022-06-30T07:21:32.185171Z","shell.execute_reply.started":"2022-06-30T07:21:32.083615Z","shell.execute_reply":"2022-06-30T07:21:32.184211Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# CV split","metadata":{"id":"9e05b6c4","papermill":{"duration":0.031837,"end_time":"2022-03-21T11:34:22.593468","exception":false,"start_time":"2022-03-21T11:34:22.561631","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# CV split\n# ====================================================\ntrain['score_map'] = train['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\nFold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train['score_map'])):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\n# display(train.groupby('fold').size())","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1644920084528,"user":{"displayName":"Yasufumi Nakama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17486303986134302670"},"user_tz":-540},"id":"3ba287c4","outputId":"307dc0e2-17d6-4bfe-9e95-fdd9b6303974","papermill":{"duration":0.052109,"end_time":"2022-03-21T11:34:22.677648","exception":false,"start_time":"2022-03-21T11:34:22.625539","status":"completed"},"tags":[],"scrolled":true,"execution":{"iopub.status.busy":"2022-06-30T04:53:35.237785Z","iopub.execute_input":"2022-06-30T04:53:35.238103Z","iopub.status.idle":"2022-06-30T04:53:35.262672Z","shell.execute_reply.started":"2022-06-30T04:53:35.238073Z","shell.execute_reply":"2022-06-30T04:53:35.261843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sec_toks(df):\n    return '[' + df['context'].str[0] + ']'\n\nif CFG.prompt_engineering == 'custom_tok':\n    train['text'] = get_sec_toks(train) + train['context_text'] + ' [SEP] '+ train['anchor'] + CFG.custom_sep_token + train['target']\n    test['text'] = get_sec_toks(test) + test['context_text'] + ' [SEP] ' + test['anchor'] + CFG.custom_sep_token + test['target']\nelif CFG.prompt_engineering == 'ctx_txt':\n    train['text'] = train['anchor'] + ' [SEP] ' + train['target'] + ' [SEP] '  + train['context_text']\n    test['text'] = test['anchor'] + ' [SEP] ' + test['target'] + ' [SEP] '  + test['context_text']\nelif CFG.prompt_engineering == None:\n    train['text'] = train['anchor'] + ' [SEP] ' + train['target'] + ' [SEP] '  + train['context']\n    test['text'] = test['anchor'] + ' [SEP] ' + test['target'] + ' [SEP] '  + test['context']\nelse:\n    raise(ValueError('CFG.prompt_engineering = {} not recognized!'.format(CFG.prompt_engineering)))\n    \n\nprint(train['text'][0])\ndisplay(train['text'].head())\ndisplay(test['text'].head())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:53:35.264303Z","iopub.execute_input":"2022-06-30T04:53:35.264604Z","iopub.status.idle":"2022-06-30T04:53:35.295337Z","shell.execute_reply.started":"2022-06-30T04:53:35.264552Z","shell.execute_reply":"2022-06-30T04:53:35.294018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer","metadata":{"id":"918a28aa","papermill":{"duration":0.032412,"end_time":"2022-03-21T11:34:22.813864","exception":false,"start_time":"2022-03-21T11:34:22.781452","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\ntokenizer = AutoTokenizer.from_pretrained(CFG.model)\n\n\n# add special tokens for sections\ncpc_sections = [\n    'A', # Human Necessities\n    'B', # Operations and Transport\n    'C', # Chemistry and Metallurgy\n    'D', # Textiles\n    'E', # Fixed Constructions\n    'F', # Mechanical Engineering\n    'G', # Physics\n    'H', # Electricity\n    'Y' # Emerging Cross-Sectional Technologies\n]\nif CFG.prompt_engineering == 'custom_tok':\n    tokenizer.add_special_tokens({'additional_special_tokens': ['['+  s + ']' for s in cpc_sections]})\n    print(tokenizer.all_special_tokens)\n    \ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCFG.tokenizer = tokenizer","metadata":{"papermill":{"duration":5.588243,"end_time":"2022-03-21T11:34:28.435013","exception":false,"start_time":"2022-03-21T11:34:22.84677","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T04:53:35.297205Z","iopub.execute_input":"2022-06-30T04:53:35.29762Z","iopub.status.idle":"2022-06-30T04:53:37.437397Z","shell.execute_reply.started":"2022-06-30T04:53:35.297551Z","shell.execute_reply":"2022-06-30T04:53:37.43657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"14da40cf","papermill":{"duration":0.034425,"end_time":"2022-03-21T11:34:28.504726","exception":false,"start_time":"2022-03-21T11:34:28.470301","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Define max_len\n# ====================================================\nlengths_dict = {}\n\nlengths = []\ntk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nlengths_dict['context_text'] = lengths\n\nfor text_col in ['anchor', 'target']:\n    lengths = []\n    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    lengths_dict[text_col] = lengths\n    \nCFG.max_len = max(lengths_dict['anchor']) + max(lengths_dict['target'])\\\n                + max(lengths_dict['context_text']) + 4 # CLS + SEP + SEP + SEP\nLOGGER.info(f\"max_len: {CFG.max_len}\")","metadata":{"executionInfo":{"elapsed":32827,"status":"ok","timestamp":1644920122500,"user":{"displayName":"Yasufumi Nakama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17486303986134302670"},"user_tz":-540},"id":"c00327b0","outputId":"26e947da-b73a-494d-e776-906b037ac08a","papermill":{"duration":26.672702,"end_time":"2022-03-21T11:34:55.211912","exception":false,"start_time":"2022-03-21T11:34:28.53921","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T04:53:37.438566Z","iopub.execute_input":"2022-06-30T04:53:37.438857Z","iopub.status.idle":"2022-06-30T04:53:39.049424Z","shell.execute_reply.started":"2022-06-30T04:53:37.438811Z","shell.execute_reply":"2022-06-30T04:53:39.048706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\ndef prepare_labels(cfg, label):\n    if cfg.classification:\n        label_onehot = [0 for _ in range(cfg.target_size)]\n        label_onehot[cfg.map_score[label]] = 1 \n        return torch.tensor(label_onehot, dtype=torch.float)\n    elif cfg.ordinal:\n        label_ordinal = [1 if i <= cfg.map_score[label] else 0 for i in range(cfg.target_size)]\n        return torch.tensor(label_ordinal, dtype=torch.float)\n    else:\n        return torch.tensor(label, dtype=torch.float)\n\nclass TrainDataset(Dataset):\n    def __init__(self, cfg, df, chem_lookup=None):\n        self.cfg = cfg\n        if cfg.chem_comp_pred_avg and chem_lookup:\n            df = chem_lookup.post_augment_chem_formulae(df)\n        self.texts = df['text'].values\n        self.labels = df['score'].values\n        self.ids = df['id'].values\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        label = prepare_labels(self.cfg, self.labels[item])\n        return inputs, label, self.ids[item]","metadata":{"id":"9f791a19","papermill":{"duration":0.053757,"end_time":"2022-03-21T11:34:55.302054","exception":false,"start_time":"2022-03-21T11:34:55.248297","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T04:53:39.050843Z","iopub.execute_input":"2022-06-30T04:53:39.051279Z","iopub.status.idle":"2022-06-30T04:53:39.066705Z","shell.execute_reply.started":"2022-06-30T04:53:39.051242Z","shell.execute_reply":"2022-06-30T04:53:39.065939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"e04d6363","papermill":{"duration":0.036359,"end_time":"2022-03-21T11:34:55.535904","exception":false,"start_time":"2022-03-21T11:34:55.499545","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"id":"4c5bab44","papermill":{"duration":0.05059,"end_time":"2022-03-21T11:34:55.622912","exception":false,"start_time":"2022-03-21T11:34:55.572322","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T04:53:39.068157Z","iopub.execute_input":"2022-06-30T04:53:39.068441Z","iopub.status.idle":"2022-06-30T04:53:39.084688Z","shell.execute_reply.started":"2022-06-30T04:53:39.068395Z","shell.execute_reply":"2022-06-30T04:53:39.083825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions","metadata":{"id":"deee9675","papermill":{"duration":0.03649,"end_time":"2022-03-21T11:34:55.940757","exception":false,"start_time":"2022-03-21T11:34:55.904267","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Helper functions\n# ====================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\ndef ordinal_regression(predictions, targets):\n    \"\"\"Ordinal regression with encoding as in https://arxiv.org/pdf/0704.1028.pdf\"\"\"\n    return nn.MSELoss(reduction='mean')(predictions, targets)\n\ndef average_by_id(df):\n    ''' Averages a dataframe by a column id'''\n    orig_id_order = df['id']\n    unordered_means = df.groupby('id').mean().reset_index()\n    return unordered_means.set_index('id').loc[orig_id_order].reset_index().drop_duplicates()\n\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n    losses = AverageMeter()\n    start = end = time.time()\n    global_step = 0\n    for step, (inputs, labels, _) in enumerate(train_loader):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n#         needed to disable amp, error: half expected got float (maybe some bug)\n        if CFG.ordinal:\n            y_preds = model(inputs)\n        else:\n            with torch.cuda.amp.autocast(enabled=CFG.apex):\n                y_preds = model(inputs)\n        if CFG.classification:\n            loss = criterion(y_preds, torch.argmax(labels, 1))\n        elif CFG.ordinal:\n            loss = criterion(y_preds, labels) \n        else:\n            loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        scaler.scale(loss).backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            global_step += 1\n            if CFG.batch_scheduler:\n                scheduler.step()\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.8f}  '\n                  .format(epoch+1, step, len(train_loader), \n                          remain=timeSince(start, float(step+1)/len(train_loader)),\n                          loss=losses,\n                          grad_norm=grad_norm,\n                          lr=scheduler.get_lr()[0]))\n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    ids = []\n    start = end = time.time()\n    for step, (inputs, labels, sample_id) in enumerate(valid_loader):\n        ids.append(sample_id)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            y_preds = model(inputs)\n            \n        if CFG.classification:\n            loss = criterion(y_preds, torch.argmax(labels, 1))\n        elif CFG.ordinal:\n            loss = criterion(y_preds, labels)\n        else:\n            loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        if CFG.classification or CFG.ordinal:\n            preds.append(y_preds.to('cpu').numpy())\n        else:\n            preds.append(y_preds.sigmoid().to('cpu').numpy())\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, len(valid_loader),\n                          loss=losses,\n                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n    predictions = np.concatenate(preds)\n    ids = np.concatenate(ids)\n    print(\"Sanity Check:\", sum(pd.Series(ids).duplicated()))\n    if CFG.classification:\n        predictions = np.argmax(predictions, axis=1)\n        predictions = np.array([CFG.map_labels[p] for p in predictions])\n    elif CFG.ordinal:\n        predictions = (predictions > 0.5).cumprod(axis=1).sum(axis=1) - 1\n        predictions = np.clip(predictions, 0, None)\n        predictions = np.array([CFG.map_labels[p] for p in predictions])\n    else:\n        predictions = np.concatenate(predictions)\n        if CFG.clipping:\n            predictions = np.clip(predictions, 0, 1)\n        if CFG.minmax:\n            predictions = minmax_scale(predictions, feature_range=(0, 1))\n            \n    if CFG.chem_comp_pred_avg:\n        pred_new = average_by_id(pd.DataFrame({'pred': predictions, 'id': ids}))\n        predictions = pred_new['pred'].to_numpy()\n    return losses.avg, predictions\n","metadata":{"id":"c8263b0c","papermill":{"duration":0.112662,"end_time":"2022-03-21T11:34:56.089768","exception":false,"start_time":"2022-03-21T11:34:55.977106","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T04:53:39.08647Z","iopub.execute_input":"2022-06-30T04:53:39.087001Z","iopub.status.idle":"2022-06-30T04:53:39.12544Z","shell.execute_reply.started":"2022-06-30T04:53:39.086965Z","shell.execute_reply":"2022-06-30T04:53:39.12455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# train loop\n# ====================================================\ndef train_loop(folds, fold):\n    \n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n    if CFG.augment_identity_graph_data != None and CFG.validate_on_original:\n        valid_folds = valid_folds[valid_folds['augmented'] == False].reset_index(drop=True)\n\n    valid_labels = valid_folds['score'].values\n    \n    train_dataset = TrainDataset(CFG, train_folds)\n    # we only want the prediction using chemical synonyms for the validation\n    valid_dataset = TrainDataset(CFG, valid_folds, chem_lookup)\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=True,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=False,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = CustomModel(CFG, config_path=None, pretrained=True)\n    torch.save(model.config, OUTPUT_DIR+'config.pth')\n    model.to(device)\n    \n    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=CFG.encoder_lr, \n                                                decoder_lr=CFG.decoder_lr,\n                                                weight_decay=CFG.weight_decay)\n    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n    \n    # ====================================================\n    # scheduler\n    # ====================================================\n    def get_scheduler(cfg, optimizer, num_train_steps):\n        if cfg.scheduler == 'linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n            )\n        elif cfg.scheduler == 'cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n            )\n        return scheduler\n    \n    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    if CFG.classification:\n        criterion = nn.CrossEntropyLoss()\n    elif CFG.ordinal:\n        criterion = ordinal_regression\n    else:\n        criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n    \n    best_score = 0.\n\n    for epoch in range(CFG.epochs):\n\n        start_time = time.time()\n\n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n\n        # eval\n        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n        \n        # scoring\n        score = get_score(valid_labels, predictions)\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')       \n        if best_score < score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(),\n                        'predictions': predictions},\n                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n\n    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n                             map_location=torch.device('cpu'))['predictions']\n    valid_folds['pred'] = predictions\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return valid_folds","metadata":{"id":"bed940e1","papermill":{"duration":0.300071,"end_time":"2022-03-21T11:34:56.447127","exception":false,"start_time":"2022-03-21T11:34:56.147056","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T04:53:39.127084Z","iopub.execute_input":"2022-06-30T04:53:39.127344Z","iopub.status.idle":"2022-06-30T04:53:39.15322Z","shell.execute_reply.started":"2022-06-30T04:53:39.127308Z","shell.execute_reply":"2022-06-30T04:53:39.152413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    def get_result(oof_df):\n        labels = oof_df['score'].values\n        preds = oof_df['pred'].values\n        score = get_score(labels, preds)\n        LOGGER.info(f'Score: {score:<.4f}')\n    \n    oof_df = pd.DataFrame()\n    for fold in range(CFG.n_fold):\n        if fold in CFG.trn_fold:\n            _oof_df = train_loop(train, fold)\n            oof_df = pd.concat([oof_df, _oof_df])\n            LOGGER.info(f\"========== fold: {fold} result ==========\")\n            get_result(_oof_df)\n    oof_df = oof_df.reset_index(drop=True)\n    LOGGER.info(f\"========== CV ==========\")\n    get_result(oof_df)\n    oof_df.to_csv(OUTPUT_DIR+'oof_df.csv')","metadata":{"id":"6cc76b1e","papermill":{"duration":6464.068626,"end_time":"2022-03-21T13:22:40.573692","exception":false,"start_time":"2022-03-21T11:34:56.505066","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T04:53:39.154659Z","iopub.execute_input":"2022-06-30T04:53:39.155113Z","iopub.status.idle":"2022-06-30T05:06:46.881502Z","shell.execute_reply.started":"2022-06-30T04:53:39.155075Z","shell.execute_reply":"2022-06-30T05:06:46.880705Z"},"trusted":true},"execution_count":null,"outputs":[]}]}